{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f9eec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f07b8a2-0123-4857-a280-d01d0335e27f",
   "metadata": {},
   "source": [
    "This Notebook contains a walk through:\n",
    "* The evaluation of BERT LLM model\n",
    "* Finetuning BERT with custom data (imdb) with different parameters\n",
    "* Compare the accuracy between the original model and after fine-tuning\n",
    "\n",
    "These tools and techiniques are used throughout the document:\n",
    "\n",
    "* LLM Model: **google-bert/bert-base-cased**\n",
    "* PEFT technique: **LoRA**\n",
    "* Evaluation approach: **Using accuracy metric**\n",
    "* Fine-tuning dataset: **stanfordnlp/imdb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "In this section, a pre-trained Hugging Face model is loaded and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e735a-101f-440e-98c3-403b1cdafb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You kight need to restart the kernel after this command to avoid errors when calling AutoModelForSequenceClassification\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d29dae-27b0-4ffa-b51e-baa9961dfcfc",
   "metadata": {},
   "source": [
    "## Prepare the Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9f8e5-75ec-47cd-b1b5-72ed07c0ff2d",
   "metadata": {},
   "source": [
    "### Load a pretrained HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2c060-d8a5-4698-95b6-3162c357f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id=\"google-bert/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd52b1d-3019-4725-b98f-5f453467bab8",
   "metadata": {},
   "source": [
    "### Load and preprocess a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB contain train, test and unsupervised datasets with 25000, 25000 and 500000 samples respectively.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_datasets = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test_datasets = dataset[\"test\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81fb54c-5169-4ad5-9382-64386c199fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = 6000\n",
    "eval_dataset_size = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train_datasets.shuffle(seed=42).select(range(train_dataset_size))\n",
    "small_eval_dataset = tokenized_test_datasets.shuffle(seed=42).select(range(eval_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e9886-23e7-4742-8698-5e330adc1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_eval_dataset, small_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0ffeb-29cb-440e-9331-c05243756291",
   "metadata": {},
   "source": [
    "### Evaluate the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d20d27-a597-4ed8-bd69-136c035717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a map between expected ids and labels\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dd356-65b0-48e8-948b-1c09230d83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c30a1f-ff16-4d72-a456-a1661f8b1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model, is_human: bool = False):\n",
    "    params: int = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params\n",
    "\n",
    "print(model)\n",
    "print(\"Total # of params for the model {}: {}\".format(model_id, count_params(model, is_human=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Generate a random integer within the range of eval_dataset_size\n",
    "x = random.randint(0, eval_dataset_size)\n",
    "\n",
    "print(\"text: {},\\nlabel:{} = {}\".format(\n",
    "    small_eval_dataset[\"text\"][x],\n",
    "    small_eval_dataset[\"label\"][x],\n",
    "    id2label[small_eval_dataset[\"label\"][x]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa2908-c9e2-4f2a-bae7-cedd7aea53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use accuracy metric\n",
    "#Function inspired from https://huggingface.co/learn/nlp-course/en/chapter3/3#evaluation\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742624c3-05dc-468a-9e8c-fe46fde89841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        \"evaluate_foundational_model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16\n",
    "    ),\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8349e-b61c-4b2b-be95-dbc3a2bd95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# Let's see the perfomance of the foundation model before any prior training\n",
    "trainer_foundation_eval=trainer.evaluate(eval_dataset=small_eval_dataset)\n",
    "trainer_foundation_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11655fb5-3f9d-477f-85a2-096631c205f8",
   "metadata": {},
   "source": [
    "###### **Without any fine tuning the model \"google-bert/bert-base-cased\" has an _accuracy_ of _0.4963_**\n",
    "###### **Not great, but we will improve it sustantialy with fione tuning in the next steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e894278-9998-4414-b4b9-ed306fee9ccb",
   "metadata": {},
   "source": [
    "### Saving the foundation model to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41fd5e-b9be-4830-b2b8-d5203df2b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the foundational model to the local directory \"foundational_model/\" \n",
    "trainer.save_model(\"foundational_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "Create two PEFT models to test two different lora_config values and compare the results between the two. Save the PEFT model weights for each training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41ac75-cb10-4fa5-b7da-18b9884c87a0",
   "metadata": {},
   "source": [
    "### PEFT model (Same foundational model for the two PEFT configuraiotns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = model_id \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_model_id,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db754fc-bb10-4657-8722-281a5d9b9ff3",
   "metadata": {},
   "source": [
    "### Create a PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e093652-ddea-489c-b930-00c91b8cb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an dictiopnary with two set of values for two trainings and performance comparaison\n",
    "peft_values= {\n",
    "    \"values1\": {\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"values2\": {\n",
    "        \"r\": 64,\n",
    "        \"lora_alpha\": 128,\n",
    "        \"lora_dropout\": 0.01,\n",
    "        \"bias\": \"none\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config1 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=peft_values[\"values1\"][\"r\"],\n",
    "    lora_alpha=peft_values[\"values1\"][\"lora_alpha\"],\n",
    "    lora_dropout=peft_values[\"values1\"][\"lora_dropout\"],\n",
    "    bias=peft_values[\"values1\"][\"bias\"],\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model1 = get_peft_model(model, lora_config1)\n",
    "lora_model1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed404b4-3103-45f7-9e7b-b66d997c7bce",
   "metadata": {},
   "source": [
    "**Here we can see the advantage of using PEFT fine tuning instead of training the whole model: only 0.543% of the 109 million parameters BERT has.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3e398-5fd0-41c1-ae5c-44f311a84d9d",
   "metadata": {},
   "source": [
    "### Train the PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46862f9-750f-405e-af9e-728051e2b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_peft1 = TrainingArguments(\n",
    "    \"trainer_peft1_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer1 = Trainer(\n",
    "    model=lora_model1,\n",
    "    args=training_args_peft1,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer1_eval=trainer1.evaluate()\n",
    "trainer1_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9af57",
   "metadata": {},
   "source": [
    "##### **With fine tuning the model1 \"google-bert/bert-base-cased\" the _accuracy_ is now _0.882_ much better than the performance of the original foundational model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b2070",
   "metadata": {},
   "source": [
    "### Save the PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model1.save_pretrained(\"trainer_peft_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltra trainer_peft_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc0b88-6ae6-453f-b8ab-75e8d0ead40c",
   "metadata": {},
   "source": [
    "### Create PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302c8ec-c10d-430d-89a2-7e7a917917ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config2 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=peft_values[\"values2\"][\"r\"],\n",
    "    lora_alpha=peft_values[\"values2\"][\"lora_alpha\"],\n",
    "    lora_dropout=peft_values[\"values2\"][\"lora_dropout\"],\n",
    "    bias=peft_values[\"values2\"][\"bias\"],\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23842433-d641-4c91-adcc-048bbb88bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model2 = get_peft_model(model, lora_config2)\n",
    "lora_model2.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc41b7-1d61-4d88-a2f9-ba536b3f1900",
   "metadata": {},
   "source": [
    "### Train PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85654d-f9a8-49e7-8bd0-f402e2ab9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_peft2 = TrainingArguments(\n",
    "    \"trainer_peft2_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c5076-a39c-4dd1-b4ff-329c9d57f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer2 = Trainer(\n",
    "    model=lora_model2,\n",
    "    args=training_args_peft2,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c837e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer2_eval=trainer2.evaluate()\n",
    "trainer2_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf6c6f",
   "metadata": {},
   "source": [
    "**With fine tuning the model2 \"google-bert/bert-base-cased\" the _accuracy_ is now _0.899_ much better than the performance of the original foundational model and the PEFT1 model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2447c43",
   "metadata": {},
   "source": [
    "### Save the PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model1.save_pretrained(\"trainer_peft_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01468279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -ltra trainer_peft_2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "Loading the PEFT model weights that has the best accuracy and evaluate the performance of the trained PEFT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefb3fe-6110-49f4-981c-7a87cb76b2e1",
   "metadata": {},
   "source": [
    "## Perform Inference Using the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679d481",
   "metadata": {},
   "source": [
    "### Load the saved PEFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6d180",
   "metadata": {},
   "source": [
    "We load the best PEFT model of the two we created: \"trainer_peft_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = AutoModelForSequenceClassification.from_pretrained(\"trainer_peft_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c08ece",
   "metadata": {},
   "source": [
    "### Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "x = random.randint(0, eval_dataset_size)\n",
    "\n",
    "text_to_classify=small_eval_dataset[\"text\"][x]\n",
    "\n",
    "print(\"Text from eval_dataset: {} \\n\\nlabel from eval_dataset:{}\".format(\n",
    "    small_eval_dataset[\"text\"][x], \n",
    "    id2label[small_eval_dataset[\"label\"][x]]\n",
    "))\n",
    "\n",
    "\n",
    "def classify(text):\n",
    "    #Tokenize the text and return a PyTorch tensor\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    #Pass the tokinezed text to the model and get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = saved_model(**inputs)\n",
    "    # Get the predicted class\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(f\"\\nPredicted class: {model.config.id2label[predictions.item()]} \\n\")\n",
    "\n",
    "\n",
    "classify(text_to_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03bc9e",
   "metadata": {},
   "source": [
    "**The inference classified the text as POSITIVE which matches the label in the dataset.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
